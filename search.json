[
  {
    "objectID": "content/Onboarding.html",
    "href": "content/Onboarding.html",
    "title": "New Employee Onboarding",
    "section": "",
    "text": "What to expect in the first 1 to 7 days\nPatience is key in the first week. Having to go through approvals, appointments and introductions your initial days may be overwhelming and delays getting fully onboarded are expected. However, as each day passes you will get your NOAA credentials - which will get you access to the building, your NOAA computer and then sort through the necessary access to servers, and get up to speed on what the archives are and what we do here at NCEI. There are required training courses from CU and NOAA that you will take online (see below) and several CU forms to complete that the CU team will walk you through..\nIn short, It’s a lot of paperwork, learning, waiting, and a bit of doing.\n\n\nWhat to expect in the first month\nAs access to all things NCEI are worked out over the first couple of weeks of the job, you should be well on your way to learning the tasks of the job, feeling integrated with the team and meeting many others in the office.\nThroughout this month, you will be given a lot of information and finding ways to document it, organize, and reference it will be very beneficial. The team has a couple of tools along with existing documentation - your help using these tools, providing feedback for what could be better, and additional information that is needed are always appreciated.\nHopefully be able to start doing your job :)))\n\n\nThings to configure\nHere are some onboarding guidelines so you know you are on the right track:Onboarding Guidelines\nIf you are a student joining the team you will most likely need to work on a Windows desktop. In order to access the servers and the mission’s systems, you will need to configure the terminal application called Putty:Putty Configuration\nAnother aspect of being a student employee includes updating your hourly timesheet each day.\nInstructions for configuration can be found here: Timesheet setup for Students\n\n\nTraining\nAs a NOAA and CU Employee, there will also be several training courses that you will be required to complete:\nNOAA training: NOAA IT Security Awareness\nNOAA training: Records Management Course 101\nCU training: Discrimination and Harassment\nCU training: Introduction to Export Controls\nOnce you’ve gotten this far, make sure you schedule a tour of the NOAA building! As one of the nation’s leaders in science and technology, the building houses several exhibits that detail the achievements of NOAA Boulder at the David Skaggs Research Center.",
    "crumbs": [
      "New Employee Onboarding"
    ]
  },
  {
    "objectID": "content/cheatsheet.html",
    "href": "content/cheatsheet.html",
    "title": "Cheatsheet",
    "section": "",
    "text": "With a lot of moving parts to the job you might find it helpful to look back on some cheat sheets. These documents will give you some help with common commands, queries, and government acronyms used within Team Fish.\n\nTerminology: This short doc provides common terms used when discussing the scientific data of the archives.\nAcronyms: This doc includes common government acronyms for agencies, data providers, and more.\nAWS Notes: These notes relay common commands used to access the AWS S3 bucket for Water Column data.\nGoogle Cloud: These notes include common commands used to access the Google Cloud (GCP) bucket holding the Passive Acoustic data.\nLINUX COMMANDS: This doc outlines the basic linux commands used to manage the data on our servers and desktops.\nSQL Cheat Sheet: This document includes many common SQL queries that we use to access data on our database and in the archive. \nInstruments: This doc outlines the types of instruments that are used routinely for water column sonar surveys and passive acoustic monitoring programs.",
    "crumbs": [
      "Cheat sheets"
    ]
  },
  {
    "objectID": "content/Archives.html",
    "href": "content/Archives.html",
    "title": "Submitting Archive Data",
    "section": "",
    "text": "In order to support as many projects and data sources as possible, the Ocean Acoustics team has developed tools to standardize the way data are submitted to the archive.\nThe tools: - Enable easy entry of metadata by data providers, which it then uses to generate machine-parseable JavaScript Object Notation (JSON) metadata records - Copy all files identified to be sent to the archive to a specified location, such as an external hard drive - Create a checksum manifest file to ensure data integrity is maintained during transit\nThe tools automatically build a “data package,” a standardized structure that conforms to the BagIt specification, that efficiently feeds into the archive’s ingest pipeline.\n\nPreparing Water Column Sonar Data to Submit to the Archive\n\nCruisePack\nCruisePack is a free, stand-alone executable designed to simplify the packaging of cruise-based data for submission to the NCEI Water Column Sonar Data Archive. It is compatible with most computers and features an intuitive Graphical User Interface for entering metadata about the cruises and datasets.\n\nThe water column sonar database primarily maintains raw (as collected) data files in the instrument’s vendor-specific format (e.g., .raw, .wcd). However, other supplemental data (sound speed profiles, tides, vessel offsets, biological data, cruise reports, etc.) are also accepted.\nTo enable discovery and access of the data through the NCEI Water Column Sonar Map Viewer, please ensure navigation datagrams are included in the sonar data files (e.g., *.wcd).\nData can be submitted through shipping external hard drives, uploading to NCEI’s FTP server, or data copy using rsync through a secure shell login (Linux).\nFor more information on sending data to the water column sonar archive, see the WCSD tab of the Submission Guidelines.\n\n\nSupported Instruments / File Formats\nNCEI can accept files from most instruments capable of collecting water column sonar data. Currently supported instruments from these manufacturers include:\n\nKongsberg\n\nAll EM instruments that record in .wcd & .kmwcd format such as:\n\nEM122\nEM2040\nEM302\nEM3002\nEM710\n\nM3 instruments that record in .imb format\n\nSimrad\n\nMultibeam instruments that record in .raw format:\n\nME70\nMS70\n\nSinglebeam instruments that record in .raw format:\n\nEK500\nEK60\nES60\nEK80\nES80\n\n\nReson\n\nMultibeam instruments that record in .s7k format:\n\nSeaBat 7125\n\n\n\nIf your raw sonar data are not in one of the supported formats or from a sonar system not listed above, email wcd.info@noaa.gov to discuss options.\n\n\n\nPreparing Passive Acoustic Data to Submit to the Archive\nRecommendations for the passive acoustic community on formatting, metadata, and submitting passive acoustic data to the NCEI Passive Acoustic Data Archive can be found in the Archive Resources tab of the NCEI Passive Acoustic Data Archive project page .\n\nPassivePacker\nPassivePacker is the standalone executable with a Graphical User Interface to support preparation of passive acoustic datasets to be submitted to the archive. This tool works well for packaging a low number (&lt;10) of datasets.\nDetailed guidance on how to use PassivePacker can be found in the web-hosted manual.\n\n\nPACE\nThe Passive Acoustic Collection Engine (PACE) tool provides a programmatic approach to packaging passive acoustic data for submission to NCEI. PACE is designed for users with a  need to submit large numbers of datasets to NCEI and/or users who manage metadata in a system such as Tethys or a relational database. PACE can be controlled via a GUI or from the command line and supports user-created metadata spreadsheets or direct integration with database systems. PACE creates data packages like those created by PassivePacker without the need for PassivePacker’s manual metadata entry. Contact the team at pad.info@noaa.gov for more information on PACE.\n\n\nFile Formats\nThe passive acoustic data archive accepts both audio files and data products. Data can come from stationary marine, mobile marine or terrestrial deployments. Currently supported data products include sound level metrics, detections, sound propagation models, and sound clips. We are open to archiving other types of data products. Contact the archive team for more information.\nAudio files need to be in a standard format, such as .wav, x.wav, .aif, .flac or .mp3. Raw instrument file formats such as .dat are not accepted. Please contact the archive if you have questions regarding audio file formats. \nData product file formats include .csv, .txt., .nc, and .wav. Similar to the audio files, proprietary, software-specific formats are not allowed.\n\n\nAudio File Compression\nPassive acoustic data are voluminous. To save space in our storage systems and minimize the data volume for deliveries to end users, we use the Free Lossless audio Codec (FLAC) to compress the audio files before archiving. The .flac file format is natively supported by most processing software. However, if a user needs files in an uncompressed format, FLAC can be used  to decompress the files. The general command to decompress is flac -d &lt;filename&gt;.flac To ensure any third-party system metadata in the files is preserved, use the --keep-foreign-metadata control flag when decompressing. Additional information on using FLAC can be found on the FLAC command line page.\nFLAC compression will reduce files 50% or more depending on file characteristics, so we encourage data providers to FLAC compress their files before submitting to the archive. This allows a lot more data to be submitted at a time and saves time during the archiving step. PLease note, there are limitations in the number of channels that the flac compression can accommodate and flac does not provide any additional compression for .mp3 formats. Use the tag -keep-foreign-metadata when compressing the files to esure any metadata added by your processing software is retained. Please contact the archive if you have questions regarding FLACing your data before submission.\n\n\n\n\nSubmitting Data Packages to the Archive\nData packages can be submitted using several different pathways depending on your needs and size of datasets.\n\nShipping external hard drives\nUploading to NCEI’s FTP server\nTransferring by cloud\n\n\n\nShipping External Hard Drives\nNaming & labeling is really important! Your external hard drive cannot contain any spaces within its name. A good practice is to replace spaces with underscores. For example: “Seagate_Expansion_Drive”\nLabel the outside of your external hard drive with the following information. Please do not put the label over the hard drive’s serial number.\n\nWhen your drive is ready to ship, please contact the team pad.info@noaa.gov for a shipping address.\n2. Uploading to NCEI’s FTP server\nThis option is only available for low volume data, &lt;200 GB. Please work with the Ocean Acoustic team to determine if submitting data via FTP will work for you. \n3. Transferring by Cloud\nIn some cases, we can archive data that are on a cloud platform Some platform requirements include free egress for the NCEI Ocean Acoustic team and the bucket is within a NOAA FISMA boundary. We’ve successfully received data and subsequently archived data from cloud buckets before and hope to expand the occurrences of this pathway as more and more of our NOAA partners move their own data holdings to a cloud environment. Please contact the archive team for more information",
    "crumbs": [
      "Submitting Archive Data"
    ]
  },
  {
    "objectID": "content/Pipeline.html",
    "href": "content/Pipeline.html",
    "title": "Our Data Pipeline",
    "section": "",
    "text": "Common Package Ingest (CPI) is the system used to archive water column sonar data (WCSD) and passive acoustic data (PAD). In addition to ingesting data, this system also creates and publishes metadata records, mints DOIs, uploads data to cloud storage, and updates databases that support data discoverability and accessibility via an online map viewer here: WCSD or PAD\nThis is the general workflow:",
    "crumbs": [
      "Our Data Pipeline"
    ]
  },
  {
    "objectID": "content/first_day.html",
    "href": "content/first_day.html",
    "title": "First Day Checklist",
    "section": "",
    "text": "How to get into the NOAA David Skaggs Research Center\n\nAt 325 Broadway, Boulder, CO\n\n\n\nDirections\nDirections for getting to the David Skaggs Research Center can be found at this link.\nFor more information visit - How to get into the NOAA office on first day\n\n\nDress Code\nTeam Fish at NCEI has a relaxed environment when it comes to the dress code. We describe ourselves as a step below business casual. No need to wear a blazer or a pencil skirt unless you experience the rare occasion when congressional staff or NOAA directors visit.\nTypically jeans, shorts, t-shirts, and sneakers are totally acceptable, but do keep in mind that our team will make fun of you if you come in wearing something inappropriate. Nevertheless, many colleagues still dress in business casual attire so feel free to dress as you prefer!\nA good rule of thumb is “wear what you want but avoid anything that you would not wear in front of your grandma.”\nAnother reminder is that despite the summer heat, the office is relatively chilly no matter the time of the year, so we always recommend bringing a small jacket or sweater just in case.\n\n\nWhat to Bring\nItems to bring are a water bottle to deal with the really dry climate, laptop and charger unless you know there will be a computer waiting for you, notebook and pen, and lunch. Emotional support stuffed animals are also welcome.",
    "crumbs": [
      "First Day Checklist"
    ]
  },
  {
    "objectID": "content/code.html",
    "href": "content/code.html",
    "title": "Rendering with Code",
    "section": "",
    "text": "You can have code (R, Python or Julia) in your qmd file. You will need to have these installed on your local computer, but presumably you do already if you are adding code to your qmd files.\nx &lt;- c(5, 15, 25, 35, 45, 55)\ny &lt;- c(5, 20, 14, 32, 22, 38)\nlm(x ~ y)\n\n\nCall:\nlm(formula = x ~ y)\n\nCoefficients:\n(Intercept)            y  \n      1.056        1.326"
  },
  {
    "objectID": "content/code.html#modify-the-github-action",
    "href": "content/code.html#modify-the-github-action",
    "title": "Rendering with Code",
    "section": "Modify the GitHub Action",
    "text": "Modify the GitHub Action\nYou will need to change the GitHub Action in .github/workflows to install these and any needed packages in order for GitHub to be able to render your webpage. The GitHub Action install R since I used that in code.qmd. If you use Python or Julia instead, then you will need to update the GitHub Action to install those.\nIf getting the GitHub Action to work is too much hassle (and that definitely happens), you can alway render locally and publish to the gh-pages branch. If you do this, make sure to delete or rename the GitHub Action to something like\nrender-and-publish.old_yml\nso GitHub does not keep trying to run it. Nothing bad will happen if you don’t do this, but if you are not using the action (because it keeps failing), then you don’t need GitHub to run it."
  },
  {
    "objectID": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "href": "content/code.html#render-locally-and-publish-to-gh-pages-branch",
    "title": "Rendering with Code",
    "section": "Render locally and publish to gh-pages branch",
    "text": "Render locally and publish to gh-pages branch\nTo render locally and push up to the gh-pages branch, open a terminal window and then cd to the directory with the Quarto project. Type this in the terminal:\nquarto render gh-pages"
  },
  {
    "objectID": "content/add-content.html",
    "href": "content/add-content.html",
    "title": "Customize",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD ### Welcome to the team! Here’s what to expect in the first 1 to 7 days: Patience is key in the first week. Having to go through approvals, appointments and introductions your initial days may be overwhelming and delays getting fully onboarded are expected. However, as each day passes you will get your NOAA credentials - which will get you access to the building, your NOAA computer and then sort through the necessary access to servers, and get up to speed on what the archives are and what we do here at NCEI. There are required training courses from CU and NOAA that you will take online (see below) and several CU forms to complete that the CU team will walk you through.. In short, It’s a lot of paperwork, learning, waiting, and a bit of doing."
  },
  {
    "objectID": "content/add-content.html#edit-and-add-your-pages",
    "href": "content/add-content.html#edit-and-add-your-pages",
    "title": "Customize",
    "section": "Edit and add your pages",
    "text": "Edit and add your pages\nEdit the qmd or md files in the content folder. qmd files can include code (R, Python, Julia) and lots of Quarto markdown bells and whistles (like call-outs, cross-references, auto-citations and much more).\nEach page should start with\n---\ntitle: your title\n---\nand the first header will be the 2nd level, so ##. Note, there are situations where you leave off\n---\ntitle: your title\n---\nand start the qmd file with a level header #, but if using the default title yaml (in the --- fence) is a good habit since it makes it easy for Quarto convert your qmd file to other formats (like into a presentation)."
  },
  {
    "objectID": "content/add-content.html#add-your-pages-the-project",
    "href": "content/add-content.html#add-your-pages-the-project",
    "title": "Customize",
    "section": "Add your pages the project",
    "text": "Add your pages the project\n\nAdd the files to _quarto.yml &gt;&gt;&gt;&gt;&gt;&gt;&gt; cfc4f4e39bbdfd6274526bc7e1ffbc0b78a54276"
  },
  {
    "objectID": "content/publishing.html",
    "href": "content/publishing.html",
    "title": "Publishing",
    "section": "",
    "text": "To get your Quarto webpage to show up with the url\nyou have a few steps."
  },
  {
    "objectID": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "href": "content/publishing.html#turn-on-github-pages-for-your-repo",
    "title": "Publishing",
    "section": "Turn on GitHub Pages for your repo",
    "text": "Turn on GitHub Pages for your repo\n\nTurn on GitHub Pages under Settings &gt; Pages . You will set pages to be made from the gh-pages branch and the root directory.\nTurn on GitHub Actions under Settings &gt; Actions &gt; General\n\nThe GitHub Action will automatically recreate your website when you push to GitHub after you do the initial gh-pages set-up"
  },
  {
    "objectID": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "href": "content/publishing.html#do-your-first-publish-to-gh-pages",
    "title": "Publishing",
    "section": "Do your first publish to gh-pages",
    "text": "Do your first publish to gh-pages\nThe first time you publish to gh-pages, you need to do so locally.\n\nOn your local computer, open a terminal window and cd to your repo directory. Here is what that cd command looks like for me. You command will look different because your local repo will be somewhere else on your computer.\n\ncd ~/Documents/GitHub/NOAA-quarto-simple\n\nPublish to the gh-pages. In the terminal type\n\nquarto publish gh-pages\nThis is going to render your webpage and then push the _site contents to the gh-pages branch."
  },
  {
    "objectID": "content/publishing.html#dont-like-using-gh-pages",
    "href": "content/publishing.html#dont-like-using-gh-pages",
    "title": "Publishing",
    "section": "Don’t like using gh-pages?",
    "text": "Don’t like using gh-pages?\nIn some cases, you don’t want your website on the gh-pages branch. For example, if you are creating releases and you want the website pages archived in that release, then you won’t want your website pages on the gh-pages branch.\nHere are the changes you need to make if you to avoid gh-pages branch.\n\nAt the top of _quarto.yml add the following:\n\nproject: \n  type: website\n  output-dir: docs\n\nOn GitHub under Settings &gt; Pages set pages to be made from the main branch and the docs directory.\nMake sure docs is not listed in .gitignore\nPublish the site the first time locally using quarto publish from the terminal\nChange the GitHub Action because you can’t use quarto publish gh-pages. You’ll need to push to the main branch yourself (in the GitHub Action)\n\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    env:\n      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}\n\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v2 \n        \n      - name: Set up R (needed for Rmd)\n        uses: r-lib/actions/setup-r@v2\n\n      - name: Install packages (needed for Rmd)\n        run: Rscript -e 'install.packages(c(\"rmarkdown\", \"knitr\", \"jsonlite\"))'\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n        with:\n          # To install LaTeX to build PDF book \n          # tinytex: true \n          # uncomment below and fill to pin a version\n          # version: 0.9.600\n      \n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n        with:\n          to: html\n\n      - name: Set up Git\n        run: |\n          git config --local user.email \"actions@github.com\"\n          git config --local user.name \"GitHub Actions\"\n\n      - name: Commit all changes and push\n        run: |\n          git add -A && git commit -m 'Build site' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CIRES Lab Manual",
    "section": "",
    "text": "This repository serves as the documentation for the CIRES Ocean Acoustics Lab Manual, built using Quarto and hosted via GitHub Pages. If you would like to replicate this setup for your projects, follow the instructions below.\nThis is a common format for documentation and includes a GitHub Action that automatically builds the website when changes are made. The NOAA palette and fonts have been added to theme.scss for consistent branding. The website will be served from the gh-pages branch, a common approach to prevent cluttering the main branch with website files.\nThe GitHub Action installs R to support R code in your .qmd or .Rmd files. There’s no need to modify existing.Rmd files unless you want to use specific Quarto features like cross-referencing.\n##GitHub Setup\n\nUse the Template Click the green “Use This Template” button to create a repository with this content. Ensure your repository is public (GitHub Pages doesn’t work for private repositories unless you have a paid account). Also, check the box to include all branches to get the gh-pages branch. \nEnable GitHub Pages Go to Settings &gt; Pages, and set the source to be the gh-pages branch and the root directory for the site. \nEnable GitHub Actions Under Settings &gt; Actions &gt; General, make sure GitHub Actions are enabled. \nEdit Repository Description Add a description and a link to the webpage in the repository’s settings. You can find the URL by going to the Settings &gt; Pages section or under the Actions tab once the website is built."
  },
  {
    "objectID": "content/rendering.html",
    "href": "content/rendering.html",
    "title": "Rendering",
    "section": "",
    "text": "The repo includes a GitHub Action that will render (build) the website automatically when you make changes to the files. It will be pushed to the gh-pages branch.\nBut when you are developing your content, you will want to render it locally."
  },
  {
    "objectID": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "href": "content/rendering.html#step-1.-make-sure-you-have-a-recent-rstudio",
    "title": "Rendering",
    "section": "Step 1. Make sure you have a recent RStudio",
    "text": "Step 1. Make sure you have a recent RStudio\nHave you updated RStudio since about August 2022? No? Then update to a newer version of RStudio. In general, you want to keep RStudio updated and it is required to have a recent version to use Quarto."
  },
  {
    "objectID": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "href": "content/rendering.html#step-2.-clone-and-create-rstudio-project",
    "title": "Rendering",
    "section": "Step 2. Clone and create RStudio project",
    "text": "Step 2. Clone and create RStudio project\nFirst, clone the repo onto your local computer. How? You can click File &gt; New Project and then select “Version Control”. Paste in the url of the repository. That will clone the repo on to your local computer. When you make changes, you will need to push those up."
  },
  {
    "objectID": "content/rendering.html#step-3.-render-within-rstudio",
    "href": "content/rendering.html#step-3.-render-within-rstudio",
    "title": "Rendering",
    "section": "Step 3. Render within RStudio",
    "text": "Step 3. Render within RStudio\nRStudio will recognize that this is a Quarto project by the presence of the _quarto.yml file and will see the “Build” tab. Click the “Render website” button to render to the _site folder.\nPreviewing: You can either click index.html in the _site folder and specify “preview in browser” or set up RStudio to preview to the viewer panel. To do the latter, go to Tools &gt; Global Options &gt; R Markdown. Then select “Show output preview in: Viewer panel”."
  },
  {
    "objectID": "content/Accessing Data.html",
    "href": "content/Accessing Data.html",
    "title": "Guidance for Accessing Data",
    "section": "",
    "text": "All publicly accessible data and products can be discovered in our dedicated map viewers.\n\nWater Column Sonar Data\nPassive Acoustic Data\n\nThe map viewers allow filtering of datasets based on various criteria and also allow data ordering. When ordering data there is the option of receiving it directly from NCEI or accessing the data from a cloud repository that is part of the NOAA Open Data Dissemination (NODD) project. The map viewer  will provide additional information to access specific files, but data can also be accessed directly from the NODD buckets using the procedures below.\nWCSD Cloud\nThe WCSD archive can be accessed on the Amazon Web Services (AWS) Cloud. Using the AWS CLI tool, you can download files from the command line using the following command structure:\naws s3 cp s3://noaa-wcsd-pds/  –recursive –no-sign-request\nPAD Cloud\n\nThe PAD archive can be accessed on Google Cloud Platform (GCP). For more information on downloading from the cloud, see this Download Guide. Using the gsutil tool, you can download files from the command line using the following command structure:\ngsutil -m cp -r gs://noaa-passive-bioacoustic/ rsync \nFTP\nReceiving data directly from NCEI generally involves our anonymous FTP server. Smaller orders are processed automatically and you will receive an email with a download link. Larger orders are processed by our data managers and posted to our FTP server. In this case, the data manager will contact you with downloading instructions.\n\nExternal Media Delivery\nFor very large , &gt;500GB, data orders, FTP delivery is not feasible. For these large orders, an external hard drive(s) of sufficient capacity has to be shipped to NCEI. We will load your data and ship the drive(s) back to you. You will be alerted if you try ordering a very large volume of data and a data manager will reach out to coordinate delivery via external media.",
    "crumbs": [
      "Guidance for Accessing Data"
    ]
  },
  {
    "objectID": "content/About.html",
    "href": "content/About.html",
    "title": "Meet the Team",
    "section": "",
    "text": "The CIRES at NCEI Ocean Acoustics Archive team (Team Fish) is composed of dedicated professionals working together to support a wide range of projects and data sources.\n\nMeet Our Team\nDr. Carrie Wall, Team Lead\nChuck Anderson, Sr Data Manager\nDr. Megan McKenna, Passive Acoustic Data Scientist\nClint Lohr, Software Engineer\nEmilie Barbattini, Jr Data Manager for WCSD\nKanishka Ghodke, Lab Manual & Visualizations\nJulia Kirk, Jr Data Manager for PAD\nDavid LaPaglia, Jr Data Manager for PAD\nGabrielle Dorsey, Jr Data Manager for Database Maintenance\n\n\n\nTEAM CAT\n\n\nThis lab manual was created by Kanishka Ghodke. For any concerns or questions, please contact me at kagh3581@colorado.edu.",
    "crumbs": [
      "Meet the team"
    ]
  },
  {
    "objectID": "content/customizing.html",
    "href": "content/customizing.html",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation.\n\n\n\nLooking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory.\n\n\n\n\n\n\n\n\ncfc4f4e39bbdfd6274526bc7e1ffbc0b78a54276"
  },
  {
    "objectID": "content/customizing.html#quarto-documentation",
    "href": "content/customizing.html#quarto-documentation",
    "title": "Customization",
    "section": "",
    "text": "Quarto allow many bells and whistles to make nice output. Read the documentation here Quarto documentation."
  },
  {
    "objectID": "content/customizing.html#examples",
    "href": "content/customizing.html#examples",
    "title": "Customization",
    "section": "",
    "text": "Looking at other people’s Quarto code is a great way to figure out how to do stuff. Most will have a link to a GitHub repo where you can see the raw code. Look for a link to edit page or see source code. This will usually be on the right. Or look for the GitHub icon somewhere.\n\nQuarto gallery\nnmfs-openscapes\nFaye lab manual\nquarto-titlepages Note the link to edit is broken. Go to repo and look in documentation directory.\n\n\n\n\n\n\n\n\ncfc4f4e39bbdfd6274526bc7e1ffbc0b78a54276"
  },
  {
    "objectID": "content/Data Overview.html",
    "href": "content/Data Overview.html",
    "title": "Overview of the Data We Work With",
    "section": "",
    "text": "The Ocean Acoustics Archive provides long-term stewardship and global access to data and products from water column sonar and passive acoustic monitoring systems.\n\nWater Column Sonar Data\nWater column sonar data are routinely collected and used by NOAA to inform fish stock assessments, characterize habitat, remotely detect methane seeps, and monitor undersea oil spills.\n\n\n\nSchool of fish\n\n\n\n\n\nSound waves transmitted from ships using sonar instruments reflect back to the ship when they have hit an object(s), such as a school of fish (Source: Barbara Ambrose, NOAA)\n\n\nThe Water Column Sonar Data Archive began through a partnership with NOAA Fisheries in 2011, staffing of the original archive team in 2013, and the operational pipeline launched in 2014.\nThe majority of archived sonar data were collected by NOAA Fisheries partners across all six Fisheries Science Centers. The archive also comprises sonar data collected by the Rolling Deck To Repository program, NOAA NOS with numerous datasets specifically from NCCOS, NOAA OAR Office of Ocean Exploration (OER) from NOAA Ship Okeanos Explorer, and several academic and international institutions.\nAn overview of how and why partners across three different NOAA Line Offices collect water column sonar data can be found in this Story Map collaboratively written with scientists across NEFSC, AFSC, NCCOS, OER, and CIRES/NCEI.\nSince its beginning in 2014, the archive has expanded to include data from a variety of sonar systems operating on vessels, stationary moorings, and autonomous platforms.\nThis expansion also supports the unique submission formats from principal providers such as the Rolling Deck To Repository program and NOAA Ship Okeanos Explorer into the archive.\nThe team supports the NOAA OMAO Data Rescue Project.\nAll publicly accessible data and products in the NCEI Water Column Sonar Data Archive can be discovered in its dedicated map viewer.\n\n\nPassive Acoustic Monitoring\nPassive acoustic monitoring is a powerful observational tool that NOAA uses to detect and characterize sounds produced by fish and marine mammals, ambient noise from physical oceanographic processes, and anthropogenic noise sources that contribute to the overall ocean soundscape.\n\n\n\nMarine sound sources\n\n\n\n\n\nInstrumentation used to record them\n\n\nThe NOAA Ocean Noise Strategy articulates the agency’s vision for addressing ocean noise impacts to the species, ecosystems, and places it is entrusted to protect and guide science and management actions towards that vision. The NCEI Passive Acoustic Data Archive is a flagship project for the Ocean Noise Strategy.\nThe NCEI Passive Acoustic Data Archive was established in 2017.\nArchived data and products are the result of long-term passive acoustic monitoring efforts and are typically cross NOAA Line Office and/or multi-agency. Some examples include the NOAA-Navy Sanctuary Soundscape Monitoring Project (SanctSound), NOPP-funded and UNH-led Atlantic Deepwater Ecosystem Observatory Network (ADEON), BOEM-funded, Cornell University-led monitoring efforts typically associated with monitoring of offshore energy development, and the NOAA-led with collaboration from Mexican scientists Long-term Investigations into Soundscapes, Trends, Ecosystems, and Noise in the Gulf of Mexico (Listen GoMex).\nBiological, physical, and anthropogenic sound sources are recorded using a variety of platforms with underwater hydrophones and recording systems from mobile to stationary to animal-borne (right).\nAll publicly accessible data and products in the NCEI Passive Acoustic Data Archive can be discovered in its dedicated map viewer.\nArticles of Interest:\n\nConservation Bioacoustics- Listening to the Heartbeat of the Earth\nThe Next Wave of Passive Acoustic Data Management: How Centralized Access Can Enhance Science\nEarth Science Data Repositories: Implementing the CARE Principles\nListening to animal behavior to understand changing ecosystems\n\n\nOther Links:\n\nSound in the Ocean\nSanctSound\nOceanNoise\nPassive Acoustic Research in the Northeast \nMammals in the Ocean \nMarine Mammal Acoustics\nOffshore Wind Energy Development\nK. Lisa Yang Center for Conservation Bioacoustics\nSoundCoop\n\nExtra resources\nYou can see a few extra resources for the data overview - Here",
    "crumbs": [
      "Overview Of Data We Work With"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to the Ocean Acoustics Lab Manual. As part of the NOAA National Centers for Environmental Information Marine Geology and Geophysics section ,our team is dedicated to preserving, monitoring, assessing, and providing public access to critical ocean acoustics data. NCEI, as the steward of the nation’s vast repository of geophysical data, oversees data that spans from the depths of the ocean to the surface of the sun, driving resilience, prosperity, and equity for both current and future generations.\nNCEI, manages an extensive archive of over 60 petabytes of data, ensuring its security, accessibility and further use for science and decision making across various sectors, to address specific challenges with informed solutions.This manual provides detailed information specific to the Ocean Acoustics Archive team, highlighting our role and contributions within the broader mission of NCEI.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/index.html#overview-of-the-archives",
    "href": "content/index.html#overview-of-the-archives",
    "title": "Introduction",
    "section": "Overview of the Archives",
    "text": "Overview of the Archives\nThis section provides an overview of the archives maintained by the Ocean Acoustics Archive Team at CIRES and NCEI.\n\nWater Column Sonar Data Archive\nNOAA collects and uses active acoustic (sonar) data for various mapping requirements, focusing on the area from near the ocean surface to the seafloor. Primary uses of this sonar data include mapping of fish schools and other mid-water marine organisms, assessing biological abundance, identifying species, and characterizing habitat . Additionally, it is used to map underwater gas seeps and remotely monitor undersea oil spills.\nThe Water Column Sonar Data Archive was established in collaboration with NOAA Fisheries and the University of Colorado in 2014. This archive preserves sonar data collected by NOAA line offices, academia, industry, and international institutions, offering free, global access to over 300+ terabytes of data. This valuable resource helps answer complex questions about our oceans and facilitates exploration beyond the data’s original collection purpose.\n\n\n\nWater column sonar data collected on the NOAA Okeanos Explorer in the North Atlantic Ocean. Sonar data are overlaid onto coastal relief model bathymetry.\n\n\nContact Us: wcd.info@noaa.gov\n\n\nPassive Acoustic Data Archive\nPassive acoustic monitoring is a powerful observational tool used by NOAA and other agencies to detect and characterize sounds produced by fish and marine mammals, ambient noise from physical oceanographic processes, and anthropogenic noise sources that contribute to the overall ocean noise environment. The Passive Acoustic Data Archive, established in 2017, supports the stewardship of these valuable data collected by NOAA and partners. The archive’s database schema was designed after Tethys, an open source temporal-spatial database for metadata related to acoustic recordings, developed by Dr.Marie Roche.\nThe archive hosts free, globally accessible raw audio files and data products, supporting scientific research and management needs such as monitoring and protecting marine mammal populations, tracking geological activity, and assessing the impacts of human noise on marine life. Notably, the NCEI Passive Acoustic Data Archive staff supported the NOAA-Navy Sanctuary Soundscape Monitoring Project (SanctSound) by stewarding hundreds of raw audio files and data products and developing an educational and interactive portal for the project.\nNotably, the NCEI Passive Acoustic Data Archive staff supported the NOAA-Navy Sanctuary Soundscape Monitoring Project (SanctSound) by stewarding hundreds of raw audio files and data products and developing an educational and interactive portal for the project.\n\n\n\nThere are three main sources of sound in the ocean: physical (green), biological (blue), and human-made (orange).Credit: NOAA NEFSC Passive Acoustic Research Group\n\n\n\n\n\nA wide range of technology is used to record underwater sound. Credit: NOAA NEFSC Passive Acoustic Research Group\n\n\nContact Us: pad.info@noaa.gov",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "content/Internal SOPs.html",
    "href": "content/Internal SOPs.html",
    "title": "Internal SOPs",
    "section": "",
    "text": "Here on the Ocean Acoustic’s team we have written out the standard operating procedures for conducting ingests for both the Water Column Sonar and the Passive Acoustic data archives. This information is not public and only NCEI employees will be able to access these google docs.",
    "crumbs": [
      "Internal SOPs"
    ]
  },
  {
    "objectID": "content/Internal SOPs.html#water-column-sonar-data",
    "href": "content/Internal SOPs.html#water-column-sonar-data",
    "title": "Internal SOPs",
    "section": "Water Column Sonar Data",
    "text": "Water Column Sonar Data\n\nInternal Pipeline Procedures\n\nFor information on the ingest process of Water Column Sonar Data, take a look at this SOP: WCSD CPI Documentation\nWCSD data packages and stats should be logged in this spreadsheet: WCSD by Ship\n\nFor interacting with WCSD data requests: WCSD Data Delivery Procedure\n##Passive Acoustic Data (PAD) {#passive-acoustic-data}\n\n\nInternal Pipeline Procedures\n\nFor overall information on the ingest process of Passive Acoustic Data, take a look at this SOP: Handling of Drives: CheckSums, Pre-ingest and Ingest\nFor a more recent version of pre-ingest: PAD Pre-Ingest 2.0 \nFor information on how to submit hard drive scanning tickets:  Handling of Drives: Scanning\nHard drives should have been logged in this spreadsheet: Hard_Drive_Tracking\nAll PAD data packages and stats should be logged in this spreadsheet: PAD Ingest Tracking \nTo fulfill For interacting with PAD data requests: PAD Data Request and Delivery - Instructions \nFor documentation on Pace, the new PAD packaging tool PACE: PACE Documentation\nFor ingesting PAD PAD Ingest SOP",
    "crumbs": [
      "Internal SOPs"
    ]
  },
  {
    "objectID": "content/Internal SOPs.html#software-development-dev",
    "href": "content/Internal SOPs.html#software-development-dev",
    "title": "Internal SOPs",
    "section": "Software Development (DEV)",
    "text": "Software Development (DEV)\n\nInternal Pipeline Procedures\nFor a general overview of Common Package Ingest (CPI) see this folder. Documentation of CPI can be found here: CPI Overview",
    "crumbs": [
      "Internal SOPs"
    ]
  },
  {
    "objectID": "content/acknowledgements.html",
    "href": "content/acknowledgements.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "This repo and GitHub Action was based on the tutorial by Openscapes quarto-website-tutorial by Julia Lowndes and Stefanie Butland."
  }
]